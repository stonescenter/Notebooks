{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Processamento_Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQNqb-Alfcyd",
        "colab_type": "code",
        "outputId": "f31c8ac2-6a46-464f-858c-caed18a9dbaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# created by Steve\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "#!pip install tweet-preprocessor\n",
        "#import preprocessor as p\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJcyBjEnKpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Happy Emoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "# sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "#Emoji \n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "#combinamos\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAValgmKfsgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declaramos funÃ§oes\n",
        "def remove_emoticons(s):\n",
        "  tweet_tokenizer = TweetTokenizer()\n",
        "  tokens = tweet_tokenizer.tokenize(s)\n",
        "  s = \" \".join([word for word in tokens if word not in emoticons])\n",
        "  return s\n",
        "  \n",
        "def remove_punct(s):\n",
        "    s = \"\".join([char for char in s if char not in string.punctuation])\n",
        "    s = re.sub('[0-9]+', '', s)\n",
        "    return s\n",
        "\n",
        "def remove_unuseful(s):\n",
        "    s = re.sub(r'http\\S+', '', s)\n",
        "    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n",
        "    s = re.sub(r'@\\S+', '', s)\n",
        "    s = re.sub('&amp', '', s)\n",
        "    s = re.sub(r'[^\\x00-\\x7F]+',' ', s)\n",
        "\n",
        "    return s\n",
        "  \n",
        "def remove_stopwords(text, lang,  are_tweets = False, domain_stopwords=[]):\n",
        "  \n",
        "  stop_words = nltk.corpus.stopwords.words(lang) # lang='portuguese' or lang='english'\n",
        "  \n",
        "  s = str(text).lower() # tudo para caixa baixa\n",
        "  #table = str.maketrans({key: None for key in string.punctuation})\n",
        "  #s = s.translate(table) # remove pontuacao\n",
        "  \n",
        "  tokens = 0\n",
        "  if are_tweets:\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokens = tweet_tokenizer.tokenize(s)\n",
        "  else:\n",
        "    tokens = word_tokenize(s) #obtem tokens\n",
        "  \n",
        "  v = [i for i in tokens if not i in stop_words and not i in domain_stopwords and not i.isdigit()] # remove stopwords\n",
        "  s = \"\"\n",
        "  for token in v:\n",
        "    s += token+\" \"\n",
        "  return s.strip()\n",
        "\n",
        "# stemming\n",
        "def stemming(text,lang, are_tweets=False):\n",
        "\n",
        "  if lang=='portuguese':\n",
        "    stemmer = nltk.stem.RSLPStemmer() # stemming para portuguese\n",
        "  else:\n",
        "    stemmer = PorterStemmer() # stemming para ingles\n",
        "\n",
        "  if are_tweets:\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokens = tweet_tokenizer.tokenize(text)\n",
        "  else:\n",
        "    tokens = word_tokenize(s) #obtem tokens\n",
        "    \n",
        "  sentence_stem = ''\n",
        "  doc_text_stems = [stemmer.stem(i) for i in tokens]\n",
        "  for stem in doc_text_stems:\n",
        "    sentence_stem += stem+\" \"\n",
        "    \n",
        "  return sentence_stem.strip()\n",
        "\n",
        "def preprocess_text(text, lang='english', are_tweets = False, domain_stopwords=[]):\n",
        "  text = remove_stopwords(text, lang, True, domain_stopwords)\n",
        "  text = remove_emoticons(text)\n",
        "  text = remove_unuseful(text)\n",
        "  text = remove_punct(text)\n",
        "  #text = stemming(text, lang, True)\n",
        "\n",
        "  if are_tweets:\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokens = tweet_tokenizer.tokenize(text)\n",
        "  else:\n",
        "    tokens = word_tokenize(text) #obtem tokens)\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def process_tweets(dataset, lang):   \n",
        "\n",
        "  tweets = []\n",
        "  for index, row in dataset.iterrows():\n",
        "\n",
        "    clean_tweet = preprocess_text(row['text'], are_tweets = True)\n",
        "\n",
        "    #The polarity score is a float within the range [-1.0, 1.0]. \n",
        "    #The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
        "    blob = TextBlob(clean_tweet)\n",
        "    Sentiment = blob.sentiment\n",
        "\n",
        "    polarity = Sentiment.polarity\n",
        "    subjectivity = Sentiment.subjectivity\n",
        "\n",
        "    new_entry = [row['date'], ['text'], clean_tweet, polarity, subjectivity]\n",
        "\n",
        "    tweets.append(new_entry)\n",
        "    \n",
        "  return tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaN8gmbyZpDm",
        "colab_type": "code",
        "outputId": "0be9724e-c71b-440b-d956-1c1d1cbd89c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "orig_text = \"The #IndustryMarketplace is the turning point for #Industry40 and brings us one step closer to a fully autonomous and decentralized #MachineEconomy. Learn more about the platform &amp; its architecture. https://t.co/pDWtdGlAAu Participate now at https://t.co/m0yHmT1OXu\"\n",
        "print(orig_text)\n",
        "text = preprocess_text(orig_text, are_tweets=True)\n",
        "print(orig_text)\n",
        "print(\"%s polarity: %s subjectivity %s\" % (text, TextBlob(text).polarity, TextBlob(text).subjectivity))\n",
        "\n",
        "\n",
        "text = \"I do not like a iphone :) :-) ;) =D\"\n",
        "text = preprocess_text(text,'english', True)\n",
        "text = stemming(text, 'english', True)\n",
        "print(\"%s | polarity: %s subjectivity %s\" % (text, TextBlob(text).polarity, TextBlob(text).subjectivity))\n",
        "\n",
        "text = \"I like a iphone,%.!>! :) :-) ;) =D\"\n",
        "text = preprocess_text(text,'english', True)\n",
        "print(\"%s | polarity: %s subjectivity %s\" % (text, TextBlob(text).polarity, TextBlob(text).subjectivity))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "like iphon d | polarity: 0.0 subjectivity 0.0\n",
            "like iphone d | polarity: 0.0 subjectivity 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXzxzfxKs-EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#columns of the csv file\n",
        "COLS = ['date', 'text', 'clean_text', 'polarity','subjectivity']\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/My Drive/data/tweets_iota.csv', encoding='utf-8')\n",
        "\n",
        "tweets = process_tweets(dataset, 'english')\n",
        "#tweets = get_tweets(username)\n",
        "df = pd.DataFrame(tweets, columns=COLS)\n",
        "\n",
        "df['date'] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "# indexamos \n",
        "df['date'] = df[\"date\"].apply( lambda df : \n",
        "datetime.datetime(year=df.year, month=df.month, day=df.day))\t\n",
        "df.set_index(df[\"date\"],inplace=True)\n",
        "\n",
        "\n",
        "sentimento = df.resample('D').mean()\n",
        "#df['polarity'].resample('D', how='mean')\n",
        "\n",
        "# remplaza valores NaN por cero en dias que nao teve tweets\n",
        "sentimento.fillna(0, inplace=True)\n",
        "sentimento.head()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5AW3ht7xDRb",
        "colab_type": "text"
      },
      "source": [
        "**Visualizamos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANn-cFT4xM4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "#df.groupby(['date']).mean()['polarity'].plot(ax=ax)\n",
        "df.resample('D').mean()['polarity'].plot(ax=ax)\n",
        "#df.resample('D').mean()['subjectivity'].plot(ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3YU0daQxpZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grabamos a .csv\n",
        "sentimento.to_csv('sentimento_iota.csv', columns=['polarity', 'subjectivity'], index=True, encoding=\"utf-8\")\n",
        "print(\"dados salvos ...\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}